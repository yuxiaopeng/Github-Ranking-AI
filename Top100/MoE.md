[Github Ranking](../README.md)
==========

## Top 100 Stars in MoE

| Ranking | Project Name | Stars | Forks | Language | Open Issues | Description | Last Commit |
| ------- | ------------ | ----- | ----- | -------- | ----------- | ----------- | ----------- |
| 1 | [vllm](https://github.com/vllm-project/vllm) | 68378 | 12857 | Python | 1694 | A high-throughput and memory-efficient inference and serving engine for LLMs | 2026-01-24T03:15:17Z |
| 2 | [LlamaFactory](https://github.com/hiyouga/LlamaFactory) | 66355 | 8070 | Python | 865 | Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024) | 2026-01-20T07:54:08Z |
| 3 | [sglang](https://github.com/sgl-project/sglang) | 22664 | 4169 | Python | 655 | SGLang is a high-performance serving framework for large language models and multimodal models. | 2026-01-24T03:59:48Z |
| 4 | [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) | 12718 | 2037 | Python | 516 | TensorRT LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and supports state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT LLM also contains components to create Python and C++ runtimes that orchestrate the inference execution in a performant way. | 2026-01-24T03:57:51Z |
| 5 | [ms-swift](https://github.com/modelscope/ms-swift) | 12316 | 1152 | Python | 738 | Use PEFT or Full-parameter to CPT/SFT/DPO/GRPO 600+ LLMs (Qwen3, Qwen3-MoE, DeepSeek-R1, GLM4.5, InternLM3, Llama4, ...) and 300+ MLLMs (Qwen3-VL, Qwen3-Omni, InternVL3.5, Ovis2.5, GLM4.5v, Llava, Phi4, ...) (AAAI 2025). | 2026-01-23T09:31:23Z |
| 6 | [Bangumi](https://github.com/czy0729/Bangumi) | 5156 | 159 | TypeScript | 25 | :electron: An unofficial https://bgm.tv ui first app client for Android and iOS, built with React Native. ä¸€ä¸ªæ— å¹¿å‘Šã€ä»¥çˆ±å¥½ä¸ºé©±åŠ¨ã€ä¸ä»¥ç›ˆåˆ©ä¸ºç›®çš„ã€ä¸“é—¨åš ACG çš„ç±»ä¼¼è±†ç“£çš„è¿½ç•ªè®°å½•ï¼Œbgm.tv ç¬¬ä¸‰æ–¹å®¢æˆ·ç«¯ã€‚ä¸ºç§»åŠ¨ç«¯é‡æ–°è®¾è®¡ï¼Œå†…ç½®å¤§é‡åŠ å¼ºçš„ç½‘é¡µç«¯éš¾ä»¥å®žçŽ°çš„åŠŸèƒ½ï¼Œä¸”æä¾›äº†ç›¸å½“çš„è‡ªå®šä¹‰é€‰é¡¹ã€‚ ç›®å‰å·²é€‚é… iOS / Androidã€‚ | 2026-01-23T23:21:29Z |
| 7 | [xtuner](https://github.com/InternLM/xtuner) | 5061 | 400 | Python | 234 | A Next-Generation Training Engine Built for Ultra-Large MoE Models | 2026-01-23T09:00:04Z |
| 8 | [trace.moe](https://github.com/soruly/trace.moe) | 4928 | 262 | None | 0 | Anime Scene Search by Image | 2026-01-06T04:08:49Z |
| 9 | [MoeKoeMusic](https://github.com/MoeKoeMusic/MoeKoeMusic) | 4822 | 314 | Vue | 3 | ä¸€æ¬¾å¼€æºç®€æ´é«˜é¢œå€¼çš„é…·ç‹—ç¬¬ä¸‰æ–¹å®¢æˆ·ç«¯ An open-source, concise, and aesthetically pleasing third-party client for KuGou that supports  Windows / macOS / Linux / Web :electron: | 2026-01-17T09:04:06Z |
| 10 | [flashinfer](https://github.com/flashinfer-ai/flashinfer) | 4743 | 663 | Python | 297 | FlashInfer: Kernel Library for LLM Serving | 2026-01-23T18:47:56Z |
| 11 | [fastllm](https://github.com/ztxz16/fastllm) | 4132 | 419 | C++ | 313 | fastllmæ˜¯åŽç«¯æ— ä¾èµ–çš„é«˜æ€§èƒ½å¤§æ¨¡åž‹æŽ¨ç†åº“ã€‚åŒæ—¶æ”¯æŒå¼ é‡å¹¶è¡ŒæŽ¨ç†ç¨ å¯†æ¨¡åž‹å’Œæ··åˆæ¨¡å¼æŽ¨ç†MOEæ¨¡åž‹ï¼Œä»»æ„10Gä»¥ä¸Šæ˜¾å¡å³å¯æŽ¨ç†æ»¡è¡€DeepSeekã€‚åŒè·¯9004/9005æœåŠ¡å™¨+å•æ˜¾å¡éƒ¨ç½²DeepSeekæ»¡è¡€æ»¡ç²¾åº¦åŽŸç‰ˆæ¨¡åž‹ï¼Œå•å¹¶å‘20tpsï¼›INT4é‡åŒ–æ¨¡åž‹å•å¹¶å‘30tpsï¼Œå¤šå¹¶å‘å¯è¾¾60+ã€‚ | 2026-01-22T09:04:37Z |
| 12 | [Moeditor](https://github.com/Moeditor/Moeditor) | 4126 | 274 | JavaScript | 106 | (discontinued) Your all-purpose markdown editor. | 2020-07-07T01:08:32Z |
| 13 | [GLM-4.5](https://github.com/zai-org/GLM-4.5) | 3961 | 414 | Python | 22 | GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models | 2026-01-20T03:58:10Z |
| 14 | [Moe-Counter](https://github.com/journey-ad/Moe-Counter) | 2717 | 276 | JavaScript | 4 | Moe counter badge with multiple themes! - å¤šç§é£Žæ ¼å¯é€‰çš„èŒèŒè®¡æ•°å™¨ | 2025-08-12T08:16:18Z |
| 15 | [MoeGoe](https://github.com/CjangCjengh/MoeGoe) | 2407 | 244 | Python | 27 | Executable file for VITS inference | 2023-08-22T07:17:37Z |
| 16 | [MoE-LLaVA](https://github.com/PKU-YuanGroup/MoE-LLaVA) | 2298 | 141 | Python | 65 | ã€TMM 2025ðŸ”¥ã€‘ Mixture-of-Experts for Large Vision-Language Models | 2025-07-15T07:59:33Z |
| 17 | [Cortex](https://github.com/qibin0506/Cortex) | 2275 | 168 | Python | 7 | ä¸ªäººæž„å»ºMoEå¤§æ¨¡åž‹ï¼šä»Žé¢„è®­ç»ƒåˆ°DPOçš„å®Œæ•´å®žè·µ | 2025-12-30T13:02:21Z |
| 18 | [ICEdit](https://github.com/River-Zhang/ICEdit) | 2066 | 115 | Python | 22 | [NeurIPS 2025] Image editing is worth a single LoRA! 0.1% training data for fantastic image editing! Surpasses GPT-4o in ID persistence~ MoE ckpt released! Only 4GB VRAM is enough to run!  | 2025-12-19T19:08:02Z |
| 19 | [MoBA](https://github.com/MoonshotAI/MoBA) | 2035 | 128 | Python | 10 | MoBA: Mixture of Block Attention for Long-Context LLMs | 2025-04-03T07:28:06Z |
| 20 | [DeepSeek-MoE](https://github.com/deepseek-ai/DeepSeek-MoE) | 1887 | 300 | Python | 17 | DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models | 2024-01-16T12:18:10Z |
| 21 | [moemail](https://github.com/beilunyang/moemail) | 1879 | 1244 | TypeScript | 35 | ä¸€ä¸ªåŸºäºŽ NextJS + Cloudflare æŠ€æœ¯æ ˆæž„å»ºçš„å¯çˆ±ä¸´æ—¶é‚®ç®±æœåŠ¡ðŸŽ‰ \| A cute temporary email service built with NextJS + Cloudflare technology stack ðŸŽ‰ | 2026-01-18T06:33:25Z |
| 22 | [fastmoe](https://github.com/laekov/fastmoe) | 1829 | 200 | Python | 27 | A fast MoE impl for PyTorch | 2025-02-10T06:04:33Z |
| 23 | [OpenMoE](https://github.com/XueFuzhao/OpenMoE) | 1655 | 83 | Python | 6 | A family of open-sourced Mixture-of-Experts (MoE) Large Language Models | 2024-03-08T15:08:26Z |
| 24 | [paimon-moe](https://github.com/MadeBaruna/paimon-moe) | 1501 | 278 | JavaScript | 303 | Your best Genshin Impact companion! Help you plan what to farm with ascension calculator and database. Also track your progress with todo and wish counter. | 2026-01-14T01:43:37Z |
| 25 | [MOE](https://github.com/Yelp/MOE) | 1319 | 140 | C++ | 170 | A global, black box optimization engine for real world metric optimization. | 2023-03-24T11:00:32Z |
| 26 | [moepush](https://github.com/beilunyang/moepush) | 1249 | 352 | TypeScript | 12 | ä¸€ä¸ªåŸºäºŽ NextJS + Cloudflare æŠ€æœ¯æ ˆæž„å»ºçš„å¯çˆ±æ¶ˆæ¯æŽ¨é€æœåŠ¡, æ”¯æŒå¤šç§æ¶ˆæ¯æŽ¨é€æ¸ é“âœ¨ | 2025-05-10T11:42:44Z |
| 27 | [SpikingBrain-7B](https://github.com/BICLab/SpikingBrain-7B) | 1248 | 169 | Python | 8 | Spiking Brain-inspired Large Models, integrating hybrid efficient attention, MoE modules and spike encoding into its architecture | 2025-12-01T11:13:32Z |
| 28 | [mixture-of-experts](https://github.com/davidmrau/mixture-of-experts) | 1222 | 111 | Python | 6 | PyTorch Re-Implementation of "The Sparsely-Gated Mixture-of-Experts Layer" by Noam Shazeer et al. https://arxiv.org/abs/1701.06538 | 2024-04-19T08:22:39Z |
| 29 | [uccl](https://github.com/uccl-project/uccl) | 1186 | 116 | C++ | 66 | UCCL is an efficient communication library for GPUs, covering collectives, P2P (e.g., KV cache transfer, RL weight transfer), and EP (e.g., GPU-driven) | 2026-01-24T01:22:09Z |
| 30 | [Aria](https://github.com/rhymes-ai/Aria) | 1083 | 86 | Jupyter Notebook | 31 | Codebase for Aria - an Open Multimodal Native MoE | 2025-01-22T03:25:37Z |
| 31 | [Uni-MoE](https://github.com/HITsz-TMG/Uni-MoE) | 1074 | 64 | Python | 26 | Uni-MoE: Lychee's Large Multimodal Model Family. | 2025-12-22T02:32:34Z |
| 32 | [llama-moe](https://github.com/pjlab-sys4nlp/llama-moe) | 1004 | 65 | Python | 6 | â›·ï¸ LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training (EMNLP 2024) | 2024-12-06T04:47:07Z |
| 33 | [MoeTTS](https://github.com/luoyily/MoeTTS) | 994 | 76 | None | 0 | Speech synthesis model /inference GUI repo for galgame characters based on Tacotron2, Hifigan, VITS and Diff-svc | 2023-03-03T07:30:05Z |
| 34 | [Tutel](https://github.com/microsoft/Tutel) | 957 | 107 | C | 54 | Tutel MoE: Optimized Mixture-of-Experts Library, Support GptOss/DeepSeek/Kimi-K2/Qwen3 using FP8/NVFP4/MXFP4 | 2025-12-21T15:20:04Z |
| 35 | [MoeMemosAndroid](https://github.com/mudkipme/MoeMemosAndroid) | 947 | 104 | Kotlin | 92 | An app to help you capture thoughts and ideas | 2026-01-05T06:06:28Z |
| 36 | [moebius](https://github.com/blocktronics/moebius) | 895 | 49 | JavaScript | 40 | Modern ANSI & ASCII Art Editor | 2024-05-02T15:54:35Z |
| 37 | [Time-MoE](https://github.com/Time-MoE/Time-MoE) | 887 | 96 | Python | 12 | [ICLR 2025 Spotlight] Official implementation of "Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts" | 2025-12-12T08:11:45Z |
| 38 | [Hunyuan-A13B](https://github.com/Tencent-Hunyuan/Hunyuan-A13B) | 811 | 120 | Python | 24 | Tencent Hunyuan A13B (short as Hunyuan-A13B), an innovative and open-source LLM built on a fine-grained MoE architecture. | 2025-07-08T08:45:27Z |
| 39 | [Adan](https://github.com/sail-sg/Adan) | 806 | 70 | Python | 4 | Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models | 2025-06-08T14:35:41Z |
| 40 | [SmartImage](https://github.com/Decimation/SmartImage) | 801 | 39 | C# | 4 | Reverse image search tool (SauceNao, IQDB, Ascii2D, trace.moe, and more) | 2026-01-17T20:37:03Z |
| 41 | [DeepSeek-671B-SFT-Guide](https://github.com/ScienceOne-AI/DeepSeek-671B-SFT-Guide) | 793 | 95 | Python | 1 | An open-source solution for full parameter fine-tuning of DeepSeek-V3/R1 671B, including complete code and scripts from training to inference, as well as some practical experiences and conclusions. (DeepSeek-V3/R1 æ»¡è¡€ç‰ˆ 671B å…¨å‚æ•°å¾®è°ƒçš„å¼€æºè§£å†³æ–¹æ¡ˆï¼ŒåŒ…å«ä»Žè®­ç»ƒåˆ°æŽ¨ç†çš„å®Œæ•´ä»£ç å’Œè„šæœ¬ï¼Œä»¥åŠå®žè·µä¸­ç§¯ç´¯ä¸€äº›ç»éªŒå’Œç»“è®ºã€‚) | 2025-03-13T03:51:33Z |
| 42 | [MixtralKit](https://github.com/open-compass/MixtralKit) | 773 | 76 | Python | 12 | A toolkit for inference and evaluation of 'mixtral-8x7b-32kseqlen' from Mistral AI | 2023-12-15T19:10:55Z |
| 43 | [miles](https://github.com/radixark/miles) | 767 | 81 | Python | 26 | Miles is an enterprise-facing reinforcement learning framework for large-scale MoE post-training and production workloads, forked from and co-evolving with slime. | 2026-01-24T01:03:55Z |
| 44 | [moe-theme.el](https://github.com/kuanyui/moe-theme.el) | 764 | 65 | Emacs Lisp | 15 | A customizable colorful eye-candy theme for Emacser. Moe, moe, kyun! | 2025-12-18T05:37:04Z |
| 45 | [moe](https://github.com/fox0430/moe) | 695 | 33 | Nim | 83 | A command line based editor inspired by Vim. Written in Nim. | 2026-01-23T20:51:14Z |
| 46 | [MoeMemos](https://github.com/mudkipme/MoeMemos) | 689 | 59 | Swift | 71 | An app to help you capture thoughts and ideas | 2025-12-01T16:20:57Z |
| 47 | [Awesome-Mixture-of-Experts-Papers](https://github.com/codecaution/Awesome-Mixture-of-Experts-Papers) | 657 | 45 | None | 1 | A curated reading list of research in Mixture-of-Experts(MoE). | 2024-10-30T07:48:14Z |
| 48 | [moedict-webkit](https://github.com/g0v/moedict-webkit) | 643 | 99 | Objective-C | 102 | èŒå…¸ç¶²ç«™ | 2026-01-17T11:05:50Z |
| 49 | [vtbs.moe](https://github.com/dd-center/vtbs.moe) | 632 | 36 | Vue | 33 | Virtual YouTubers in bilibili | 2025-07-31T13:39:09Z |
| 50 | [MiniMind-in-Depth](https://github.com/hans0809/MiniMind-in-Depth) | 624 | 59 | None | 5 | è½»é‡çº§å¤§è¯­è¨€æ¨¡åž‹MiniMindçš„æºç è§£è¯»ï¼ŒåŒ…å«tokenizerã€RoPEã€MoEã€KV Cacheã€pretrainingã€SFTã€LoRAã€DPOç­‰å®Œæ•´æµç¨‹ | 2025-06-16T14:13:15Z |
| 51 | [MoeList](https://github.com/axiel7/MoeList) | 619 | 20 | Kotlin | 30 | Another unofficial Android MAL client | 2026-01-19T14:53:12Z |
| 52 | [satania.moe](https://github.com/Pizzacus/satania.moe) | 611 | 55 | HTML | 3 | Satania IS the BEST waifu, no really, she is, if you don't believe me, this website will convince you | 2022-10-09T23:19:01Z |
| 53 | [moebius](https://github.com/robconery/moebius) | 610 | 42 | Elixir | 3 | A functional query tool for Elixir | 2024-10-23T18:55:45Z |
| 54 | [Chinese-Mixtral](https://github.com/ymcui/Chinese-Mixtral) | 609 | 43 | Python | 0 | ä¸­æ–‡Mixtralæ··åˆä¸“å®¶å¤§æ¨¡åž‹ï¼ˆChinese Mixtral MoE LLMsï¼‰ | 2024-04-30T04:29:06Z |
| 55 | [moebooru](https://github.com/moebooru/moebooru) | 584 | 81 | Ruby | 27 | Moebooru, a fork of danbooru1 that has been heavily modified | 2025-11-18T06:29:28Z |
| 56 | [MoeGoe_GUI](https://github.com/CjangCjengh/MoeGoe_GUI) | 572 | 68 | C# | 8 | GUI for MoeGoe | 2023-08-22T07:32:08Z |
| 57 | [sonic-moe](https://github.com/Dao-AILab/sonic-moe) | 559 | 46 | Python | 9 | Accelerating MoE with IO and Tile-aware Optimizations | 2026-01-19T12:44:27Z |
| 58 | [trace.moe-telegram-bot](https://github.com/soruly/trace.moe-telegram-bot) | 546 | 78 | TypeScript | 0 | This Telegram Bot can tell the anime when you send an screenshot to it | 2026-01-22T15:50:31Z |
| 59 | [moerail](https://github.com/Arnie97/moerail) | 506 | 41 | JavaScript | 19 | é“è·¯è½¦ç«™ä»£ç æŸ¥è¯¢ Ã— åŠ¨è½¦ç»„äº¤è·¯æŸ¥è¯¢ | 2025-08-13T12:55:25Z |
| 60 | [LPLB](https://github.com/deepseek-ai/LPLB) | 491 | 32 | Python | 0 | An early research stage expert-parallel load balancer for MoE models based on linear programming. | 2025-11-19T07:20:35Z |
| 61 | [step_into_llm](https://github.com/mindspore-lab/step_into_llm) | 482 | 127 | Jupyter Notebook | 27 | MindSpore online courses: Step into LLM | 2025-12-22T11:46:46Z |
| 62 | [InferenceMAX](https://github.com/InferenceMAX/InferenceMAX) | 427 | 72 | Python | 59 | Open Source Continuous Inference Benchmarking - GB200 NVL72 vs MI355X vs B200 vs H200 vs MI325X & soonâ„¢ TPUv6e/v7/Trainium2/3/GB300 NVL72 - DeepSeek 670B MoE, GPTOSS | 2026-01-23T23:19:43Z |
| 63 | [MOE](https://github.com/google/MOE) | 422 | 76 | Java | 18 | Make Opensource Easy - tools for synchronizing repositories | 2022-06-20T22:41:08Z |
| 64 | [hydra-moe](https://github.com/SkunkworksAI/hydra-moe) | 416 | 16 | Python | 10 | None | 2023-11-02T22:53:15Z |
| 65 | [DiT-MoE](https://github.com/feizc/DiT-MoE) | 414 | 19 | Python | 7 | Scaling Diffusion Transformers with Mixture of Experts | 2024-09-09T02:12:12Z |
| 66 | [MoeLoaderP](https://github.com/xplusky/MoeLoaderP) | 396 | 26 | C# | 11 | ðŸ–¼äºŒæ¬¡å…ƒå›¾ç‰‡ä¸‹è½½å™¨ Pics downloader for booru sites,Pixiv.net,Bilibili.com,Konachan.com,Yande.re , behoimi.org, safebooru, danbooru,Gelbooru,SankakuComplex,Kawainyan,MiniTokyo,e-shuushuu,Zerochan,WorldCosplay ,Yuriimg etc. | 2025-05-19T13:20:58Z |
| 67 | [Awesome-Efficient-Arch](https://github.com/weigao266/Awesome-Efficient-Arch) | 388 | 34 | None | 0 | Speed Always Wins: A Survey on Efficient Architectures for Large Language Models | 2025-11-11T09:47:37Z |
| 68 | [st-moe-pytorch](https://github.com/lucidrains/st-moe-pytorch) | 377 | 33 | Python | 4 | Implementation of ST-Moe, the latest incarnation of MoE after years of research at Brain, in Pytorch | 2024-06-17T00:48:47Z |
| 69 | [moe-sticker-bot](https://github.com/star-39/moe-sticker-bot) | 375 | 37 | Go | 32 | A Telegram bot that imports LINE/kakao stickers or creates/manages new sticker set. | 2024-06-06T15:28:28Z |
| 70 | [WThermostatBeca](https://github.com/fashberg/WThermostatBeca) | 370 | 73 | C++ | 4 | Open Source firmware replacement for Tuya Wifi Thermostate from Beca and Moes with Home Assistant Autodiscovery | 2023-08-26T22:10:38Z |
| 71 | [pixiv.moe](https://github.com/kokororin/pixiv.moe) | 369 | 41 | TypeScript | 0 | ðŸ˜˜ A pinterest-style layout site, shows illusts on pixiv.net order by popularity. | 2023-03-08T06:54:34Z |
| 72 | [MoeSR](https://github.com/TeamMoeAI/MoeSR) | 368 | 9 | JavaScript | 6 | An application specialized in image super-resolution for ACGN illustrations and Visual Novel CG. ä¸“æ³¨äºŽæ’ç”»/Galgame CGç­‰ACGNé¢†åŸŸçš„å›¾åƒè¶…åˆ†è¾¨çŽ‡çš„åº”ç”¨ | 2025-12-22T14:14:38Z |
| 73 | [MOEAFramework](https://github.com/MOEAFramework/MOEAFramework) | 352 | 128 | Java | 0 | A Free and Open Source Java Framework for Multiobjective Optimization | 2026-01-21T16:26:02Z |
| 74 | [notify.moe](https://github.com/animenotifier/notify.moe) | 351 | 45 | Go | 86 | :dancer: Anime tracker, database and community. Moved to https://git.akyoto.dev/web/notify.moe | 2022-09-26T07:15:05Z |
| 75 | [soft-moe-pytorch](https://github.com/lucidrains/soft-moe-pytorch) | 343 | 10 | Python | 4 | Implementation of Soft MoE, proposed by Brain's Vision team, in Pytorch | 2025-04-02T12:47:40Z |
| 76 | [dialogue.moe](https://github.com/windrises/dialogue.moe) | 342 | 11 | Python | 1 | None | 2022-12-14T14:50:38Z |
| 77 | [awesome-moe-inference](https://github.com/MoE-Inf/awesome-moe-inference) | 338 | 11 | None | 0 | Curated collection of papers in MoE model inference  | 2025-10-20T01:30:05Z |
| 78 | [nmoe](https://github.com/Noumena-Network/nmoe) | 329 | 29 | Python | 2 | MoE training for Me and You and maybe other people  | 2026-01-03T03:56:16Z |
| 79 | [YOLO-Master](https://github.com/Tencent/YOLO-Master) | 326 | 37 | Python | 14 | ðŸš€ðŸš€ðŸš€Official code for the paper   "YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection."   *(YOLO = You Only Look Once)* ðŸ”¥ðŸ”¥ðŸ”¥ | 2026-01-20T13:39:18Z |
| 80 | [moell-blog](https://github.com/moell-peng/moell-blog) | 303 | 80 | PHP | 2 | åŸºäºŽ Laravel å¼€å‘ï¼Œæ”¯æŒ Markdown è¯­æ³•çš„åšå®¢ | 2022-07-31T11:51:54Z |
| 81 | [MoH](https://github.com/SkyworkAI/MoH) | 301 | 15 | Python | 5 | MoH: Multi-Head Attention as Mixture-of-Head Attention | 2024-10-29T15:22:54Z |
| 82 | [moeSS](https://github.com/wzxjohn/moeSS) | 298 | 107 | PHP | 11 | moe SS Front End for https://github.com/mengskysama/shadowsocks/tree/manyuser | 2015-02-27T08:44:30Z |
| 83 | [moe](https://github.com/MoeOrganization/moe) | 278 | 47 | Scala | 18 | An -OFun prototype of an Ultra Modern Perl 5 | 2013-09-27T18:39:18Z |
| 84 | [android-app](https://github.com/LISTEN-moe/android-app) | 278 | 27 | Kotlin | 5 | Official LISTEN.moe Android app | 2026-01-19T14:33:55Z |
| 85 | [MoE-Infinity](https://github.com/EfficientMoE/MoE-Infinity) | 276 | 22 | Python | 13 | PyTorch library for cost-effective, fast and easy serving of MoE models. | 2025-10-15T17:52:58Z |
| 86 | [parameter-efficient-moe](https://github.com/Cohere-Labs-Community/parameter-efficient-moe) | 274 | 16 | Python | 1 | None | 2023-10-31T19:21:15Z |
| 87 | [Cornell-MOE](https://github.com/wujian16/Cornell-MOE) | 273 | 64 | C++ | 25 | A Python library for the state-of-the-art Bayesian optimization algorithms, with the core implemented in C++. | 2020-02-04T18:39:37Z |
| 88 | [MoE-Adapters4CL](https://github.com/JiazuoYu/MoE-Adapters4CL) | 269 | 24 | Python | 7 | Code for paper "Boosting Continual Learning of Vision-Language Models via Mixture-of-Experts Adapters" CVPR2024 | 2025-09-18T08:38:29Z |
| 89 | [GRIN-MoE](https://github.com/microsoft/GRIN-MoE) | 264 | 13 | None | 0 | GRadient-INformed MoE | 2024-09-25T18:46:48Z |
| 90 | [MoE-plus-plus](https://github.com/SkyworkAI/MoE-plus-plus) | 264 | 13 | Python | 1 | [ICLR 2025] MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation Experts | 2024-10-16T06:21:31Z |
| 91 | [fiddler](https://github.com/efeslab/fiddler) | 257 | 30 | Python | 2 | [ICLR'25] Fast Inference of MoE Models with CPU-GPU Orchestration | 2024-11-18T00:25:45Z |
| 92 | [Ling-V2](https://github.com/inclusionAI/Ling-V2) | 252 | 17 | Python | 5 | Ling-V2 is a MoE LLM provided and open-sourced by InclusionAI. | 2025-10-04T06:15:38Z |
| 93 | [inferflow](https://github.com/inferflow/inferflow) | 251 | 23 | C++ | 8 | Inferflow is an efficient and highly configurable inference engine for large language models (LLMs). | 2024-03-15T06:52:33Z |
| 94 | [MoeQuest](https://github.com/HotBitmapGG/MoeQuest) | 250 | 76 | Java | 1 | The meizi of a material design style welfare App. | 2017-02-14T14:13:53Z |
| 95 | [MoeLoader-Delta](https://github.com/usaginya/MoeLoader-Delta) | 246 | 36 | C# | 52 | Improved branching version of MoeLoader | 2021-07-22T20:47:41Z |
| 96 | [moeins](https://github.com/iAJue/moeins) | 244 | 70 | PHP | 2 | èŒéŸ³å½±è§† - åœ¨çº¿å½±è§†åº”ç”¨ | 2018-10-31T01:47:27Z |
| 97 | [Ling](https://github.com/inclusionAI/Ling) | 238 | 20 | Python | 2 | Ling is a MoE LLM provided and open-sourced by InclusionAI.  | 2025-05-14T06:34:57Z |
| 98 | [gdx-pay](https://github.com/libgdx/gdx-pay) | 238 | 87 | Java | 7 | A libGDX cross-platform API for InApp purchasing. | 2025-12-29T06:13:40Z |
| 99 | [moebius](https://github.com/moebiusphp/moebius) | 232 | 4 | PHP | 0 | True coroutines for PHP>=8.1 without worrying about event loops and callbacks. | 2022-06-08T23:18:45Z |
| 100 | [CoE](https://github.com/ZihanWang314/CoE) | 227 | 28 | Python | 3 | Chain of Experts (CoE) enables communication between experts within Mixture-of-Experts (MoE) models | 2025-11-04T14:49:21Z |

