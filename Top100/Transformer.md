[Github Ranking](../README.md)
==========

## Top 100 Stars in Transformer

| Ranking | Project Name | Stars | Forks | Language | Open Issues | Description | Last Commit |
| ------- | ------------ | ----- | ----- | -------- | ----------- | ----------- | ----------- |
| 1 | [transformers](https://github.com/huggingface/transformers) | 146252 | 29495 | Python | 1074 | ğŸ¤— Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training.  | 2025-06-30T19:55:36Z |
| 2 | [funNLP](https://github.com/fighting41love/funNLP) | 74458 | 14899 | Python | 33 | ä¸­è‹±æ–‡æ•æ„Ÿè¯ã€è¯­è¨€æ£€æµ‹ã€ä¸­å¤–æ‰‹æœº/ç”µè¯å½’å±åœ°/è¿è¥å•†æŸ¥è¯¢ã€åå­—æ¨æ–­æ€§åˆ«ã€æ‰‹æœºå·æŠ½å–ã€èº«ä»½è¯æŠ½å–ã€é‚®ç®±æŠ½å–ã€ä¸­æ—¥æ–‡äººååº“ã€ä¸­æ–‡ç¼©å†™åº“ã€æ‹†å­—è¯å…¸ã€è¯æ±‡æƒ…æ„Ÿå€¼ã€åœç”¨è¯ã€ååŠ¨è¯è¡¨ã€æš´æè¯è¡¨ã€ç¹ç®€ä½“è½¬æ¢ã€è‹±æ–‡æ¨¡æ‹Ÿä¸­æ–‡å‘éŸ³ã€æ±ªå³°æ­Œè¯ç”Ÿæˆå™¨ã€èŒä¸šåç§°è¯åº“ã€åŒä¹‰è¯åº“ã€åä¹‰è¯åº“ã€å¦å®šè¯åº“ã€æ±½è½¦å“ç‰Œè¯åº“ã€æ±½è½¦é›¶ä»¶è¯åº“ã€è¿ç»­è‹±æ–‡åˆ‡å‰²ã€å„ç§ä¸­æ–‡è¯å‘é‡ã€å…¬å¸åå­—å¤§å…¨ã€å¤è¯—è¯åº“ã€ITè¯åº“ã€è´¢ç»è¯åº“ã€æˆè¯­è¯åº“ã€åœ°åè¯åº“ã€å†å²åäººè¯åº“ã€è¯—è¯è¯åº“ã€åŒ»å­¦è¯åº“ã€é¥®é£Ÿè¯åº“ã€æ³•å¾‹è¯åº“ã€æ±½è½¦è¯åº“ã€åŠ¨ç‰©è¯åº“ã€ä¸­æ–‡èŠå¤©è¯­æ–™ã€ä¸­æ–‡è°£è¨€æ•°æ®ã€ç™¾åº¦ä¸­æ–‡é—®ç­”æ•°æ®é›†ã€å¥å­ç›¸ä¼¼åº¦åŒ¹é…ç®—æ³•é›†åˆã€bertèµ„æºã€æ–‡æœ¬ç”Ÿæˆ&æ‘˜è¦ç›¸å…³å·¥å…·ã€cocoNLPä¿¡æ¯æŠ½å–å·¥å…·ã€å›½å†…ç”µè¯å·ç æ­£åˆ™åŒ¹é…ã€æ¸…åå¤§å­¦XLORE:ä¸­è‹±æ–‡è·¨è¯­è¨€ç™¾ç§‘çŸ¥è¯†å›¾è°±ã€æ¸…åå¤§å­¦äººå·¥æ™ºèƒ½æŠ€æœ¯ç³»åˆ—æŠ¥å‘Šã€è‡ªç„¶è¯­è¨€ç”Ÿæˆã€NLUå¤ªéš¾äº†ç³»åˆ—ã€è‡ªåŠ¨å¯¹è”æ•°æ®åŠæœºå™¨äººã€ç”¨æˆ·åé»‘åå•åˆ—è¡¨ã€ç½ªåæ³•åŠ¡åè¯åŠåˆ†ç±»æ¨¡å‹ã€å¾®ä¿¡å…¬ä¼—å·è¯­æ–™ã€cs224næ·±åº¦å­¦ä¹ è‡ªç„¶è¯­è¨€å¤„ç†è¯¾ç¨‹ã€ä¸­æ–‡æ‰‹å†™æ±‰å­—è¯†åˆ«ã€ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç† è¯­æ–™/æ•°æ®é›†ã€å˜é‡å‘½åç¥å™¨ã€åˆ†è¯è¯­æ–™åº“+ä»£ç ã€ä»»åŠ¡å‹å¯¹è¯è‹±æ–‡æ•°æ®é›†ã€ASR è¯­éŸ³æ•°æ®é›† + åŸºäºæ·±åº¦å­¦ä¹ çš„ä¸­æ–‡è¯­éŸ³è¯†åˆ«ç³»ç»Ÿã€ç¬‘å£°æ£€æµ‹å™¨ã€Microsoftå¤šè¯­è¨€æ•°å­—/å•ä½/å¦‚æ—¥æœŸæ—¶é—´è¯†åˆ«åŒ…ã€ä¸­åæ–°åå­—å…¸æ•°æ®åº“åŠapi(åŒ…æ‹¬å¸¸ç”¨æ­‡åè¯­ã€æˆè¯­ã€è¯è¯­å’Œæ±‰å­—)ã€æ–‡æ¡£å›¾è°±è‡ªåŠ¨ç”Ÿæˆã€SpaCy ä¸­æ–‡æ¨¡å‹ã€Common Voiceè¯­éŸ³è¯†åˆ«æ•°æ®é›†æ–°ç‰ˆã€ç¥ç»ç½‘ç»œå…³ç³»æŠ½å–ã€åŸºäºbertçš„å‘½åå®ä½“è¯†åˆ«ã€å…³é”®è¯(Keyphrase)æŠ½å–åŒ…pkeã€åŸºäºåŒ»ç–—é¢†åŸŸçŸ¥è¯†å›¾è°±çš„é—®ç­”ç³»ç»Ÿã€åŸºäºä¾å­˜å¥æ³•ä¸è¯­ä¹‰è§’è‰²æ ‡æ³¨çš„äº‹ä»¶ä¸‰å…ƒç»„æŠ½å–ã€ä¾å­˜å¥æ³•åˆ†æ4ä¸‡å¥é«˜è´¨é‡æ ‡æ³¨æ•°æ®ã€cnocrï¼šç”¨æ¥åšä¸­æ–‡OCRçš„Python3åŒ…ã€ä¸­æ–‡äººç‰©å…³ç³»çŸ¥è¯†å›¾è°±é¡¹ç›®ã€ä¸­æ–‡nlpç«èµ›é¡¹ç›®åŠä»£ç æ±‡æ€»ã€ä¸­æ–‡å­—ç¬¦æ•°æ®ã€speech-aligner: ä»â€œäººå£°è¯­éŸ³â€åŠå…¶â€œè¯­è¨€æ–‡æœ¬â€äº§ç”ŸéŸ³ç´ çº§åˆ«æ—¶é—´å¯¹é½æ ‡æ³¨çš„å·¥å…·ã€AmpliGraph: çŸ¥è¯†å›¾è°±è¡¨ç¤ºå­¦ä¹ (Python)åº“ï¼šçŸ¥è¯†å›¾è°±æ¦‚å¿µé“¾æ¥é¢„æµ‹ã€Scattertext æ–‡æœ¬å¯è§†åŒ–(python)ã€è¯­è¨€/çŸ¥è¯†è¡¨ç¤ºå·¥å…·ï¼šBERT & ERNIEã€ä¸­æ–‡å¯¹æ¯”è‹±æ–‡è‡ªç„¶è¯­è¨€å¤„ç†NLPçš„åŒºåˆ«ç»¼è¿°ã€Synonymsä¸­æ–‡è¿‘ä¹‰è¯å·¥å…·åŒ…ã€HarvestTexté¢†åŸŸè‡ªé€‚åº”æ–‡æœ¬æŒ–æ˜å·¥å…·ï¼ˆæ–°è¯å‘ç°-æƒ…æ„Ÿåˆ†æ-å®ä½“é“¾æ¥ç­‰ï¼‰ã€word2wordï¼š(Python)æ–¹ä¾¿æ˜“ç”¨çš„å¤šè¯­è¨€è¯-è¯å¯¹é›†ï¼š62ç§è¯­è¨€/3,564ä¸ªå¤šè¯­è¨€å¯¹ã€è¯­éŸ³è¯†åˆ«è¯­æ–™ç”Ÿæˆå·¥å…·ï¼šä»å…·æœ‰éŸ³é¢‘/å­—å¹•çš„åœ¨çº¿è§†é¢‘åˆ›å»ºè‡ªåŠ¨è¯­éŸ³è¯†åˆ«(ASR)è¯­æ–™åº“ã€æ„å»ºåŒ»ç–—å®ä½“è¯†åˆ«çš„æ¨¡å‹ï¼ˆåŒ…å«è¯å…¸å’Œè¯­æ–™æ ‡æ³¨ï¼‰ã€å•æ–‡æ¡£éç›‘ç£çš„å…³é”®è¯æŠ½å–ã€Kashgariä¸­ä½¿ç”¨gpt-2è¯­è¨€æ¨¡å‹ã€å¼€æºçš„é‡‘èæŠ•èµ„æ•°æ®æå–å·¥å…·ã€æ–‡æœ¬è‡ªåŠ¨æ‘˜è¦åº“TextTeaser: ä»…æ”¯æŒè‹±æ–‡ã€äººæ°‘æ—¥æŠ¥è¯­æ–™å¤„ç†å·¥å…·é›†ã€ä¸€äº›å…³äºè‡ªç„¶è¯­è¨€çš„åŸºæœ¬æ¨¡å‹ã€åŸºäº14Wæ­Œæ›²çŸ¥è¯†åº“çš„é—®ç­”å°è¯•--åŠŸèƒ½åŒ…æ‹¬æ­Œè¯æ¥é¾™andå·²çŸ¥æ­Œè¯æ‰¾æ­Œæ›²ä»¥åŠæ­Œæ›²æ­Œæ‰‹æ­Œè¯ä¸‰è§’å…³ç³»çš„é—®ç­”ã€åŸºäºSiamese bilstmæ¨¡å‹çš„ç›¸ä¼¼å¥å­åˆ¤å®šæ¨¡å‹å¹¶æä¾›è®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†ã€ç”¨Transformerç¼–è§£ç æ¨¡å‹å®ç°çš„æ ¹æ®Hacker Newsæ–‡ç« æ ‡é¢˜è‡ªåŠ¨ç”Ÿæˆè¯„è®ºã€ç”¨BERTè¿›è¡Œåºåˆ—æ ‡è®°å’Œæ–‡æœ¬åˆ†ç±»çš„æ¨¡æ¿ä»£ç ã€LitBankï¼šNLPæ•°æ®é›†â€”â€”æ”¯æŒè‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—äººæ–‡å­¦ç§‘ä»»åŠ¡çš„100éƒ¨å¸¦æ ‡è®°è‹±æ–‡å°è¯´è¯­æ–™ã€ç™¾åº¦å¼€æºçš„åŸºå‡†ä¿¡æ¯æŠ½å–ç³»ç»Ÿã€è™šå‡æ–°é—»æ•°æ®é›†ã€Facebook: LAMAè¯­è¨€æ¨¡å‹åˆ†æï¼Œæä¾›Transformer-XL/BERT/ELMo/GPTé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„ç»Ÿä¸€è®¿é—®æ¥å£ã€CommonsenseQAï¼šé¢å‘å¸¸è¯†çš„è‹±æ–‡QAæŒ‘æˆ˜ã€ä¸­æ–‡çŸ¥è¯†å›¾è°±èµ„æ–™ã€æ•°æ®åŠå·¥å…·ã€å„å¤§å…¬å¸å†…éƒ¨é‡Œå¤§ç‰›åˆ†äº«çš„æŠ€æœ¯æ–‡æ¡£ PDF æˆ–è€… PPTã€è‡ªç„¶è¯­è¨€ç”ŸæˆSQLè¯­å¥ï¼ˆè‹±æ–‡ï¼‰ã€ä¸­æ–‡NLPæ•°æ®å¢å¼ºï¼ˆEDAï¼‰å·¥å…·ã€è‹±æ–‡NLPæ•°æ®å¢å¼ºå·¥å…· ã€åŸºäºåŒ»è¯çŸ¥è¯†å›¾è°±çš„æ™ºèƒ½é—®ç­”ç³»ç»Ÿã€äº¬ä¸œå•†å“çŸ¥è¯†å›¾è°±ã€åŸºäºmongodbå­˜å‚¨çš„å†›äº‹é¢†åŸŸçŸ¥è¯†å›¾è°±é—®ç­”é¡¹ç›®ã€åŸºäºè¿œç›‘ç£çš„ä¸­æ–‡å…³ç³»æŠ½å–ã€è¯­éŸ³æƒ…æ„Ÿåˆ†æã€ä¸­æ–‡ULMFiT-æƒ…æ„Ÿåˆ†æ-æ–‡æœ¬åˆ†ç±»-è¯­æ–™åŠæ¨¡å‹ã€ä¸€ä¸ªæ‹ç…§åšé¢˜ç¨‹åºã€ä¸–ç•Œå„å›½å¤§è§„æ¨¡äººååº“ã€ä¸€ä¸ªåˆ©ç”¨æœ‰è¶£ä¸­æ–‡è¯­æ–™åº“ qingyun è®­ç»ƒå‡ºæ¥çš„ä¸­æ–‡èŠå¤©æœºå™¨äººã€ä¸­æ–‡èŠå¤©æœºå™¨äººseqGANã€çœå¸‚åŒºé•‡è¡Œæ”¿åŒºåˆ’æ•°æ®å¸¦æ‹¼éŸ³æ ‡æ³¨ã€æ•™è‚²è¡Œä¸šæ–°é—»è¯­æ–™åº“åŒ…å«è‡ªåŠ¨æ–‡æ‘˜åŠŸèƒ½ã€å¼€æ”¾äº†å¯¹è¯æœºå™¨äºº-çŸ¥è¯†å›¾è°±-è¯­ä¹‰ç†è§£-è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·åŠæ•°æ®ã€ä¸­æ–‡çŸ¥è¯†å›¾è°±ï¼šåŸºäºç™¾åº¦ç™¾ç§‘ä¸­æ–‡é¡µé¢-æŠ½å–ä¸‰å…ƒç»„ä¿¡æ¯-æ„å»ºä¸­æ–‡çŸ¥è¯†å›¾è°±ã€masr: ä¸­æ–‡è¯­éŸ³è¯†åˆ«-æä¾›é¢„è®­ç»ƒæ¨¡å‹-é«˜è¯†åˆ«ç‡ã€PythonéŸ³é¢‘æ•°æ®å¢å¹¿åº“ã€ä¸­æ–‡å…¨è¯è¦†ç›–BERTåŠä¸¤ä»½é˜…è¯»ç†è§£æ•°æ®ã€ConvLabï¼šå¼€æºå¤šåŸŸç«¯åˆ°ç«¯å¯¹è¯ç³»ç»Ÿå¹³å°ã€ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†æ•°æ®é›†ã€åŸºäºæœ€æ–°ç‰ˆæœ¬rasaæ­å»ºçš„å¯¹è¯ç³»ç»Ÿã€åŸºäºTensorFlowå’ŒBERTçš„ç®¡é“å¼å®ä½“åŠå…³ç³»æŠ½å–ã€ä¸€ä¸ªå°å‹çš„è¯åˆ¸çŸ¥è¯†å›¾è°±/çŸ¥è¯†åº“ã€å¤ç›˜æ‰€æœ‰NLPæ¯”èµ›çš„TOPæ–¹æ¡ˆã€OpenCLaPï¼šå¤šé¢†åŸŸå¼€æºä¸­æ–‡é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä»“åº“ã€UERï¼šåŸºäºä¸åŒè¯­æ–™+ç¼–ç å™¨+ç›®æ ‡ä»»åŠ¡çš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹ä»“åº“ã€ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†å‘é‡åˆé›†ã€åŸºäºé‡‘è-å¸æ³•é¢†åŸŸ(å…¼æœ‰é—²èŠæ€§è´¨)çš„èŠå¤©æœºå™¨äººã€g2pCï¼šåŸºäºä¸Šä¸‹æ–‡çš„æ±‰è¯­è¯»éŸ³è‡ªåŠ¨æ ‡è®°æ¨¡å—ã€Zincbase çŸ¥è¯†å›¾è°±æ„å»ºå·¥å…·åŒ…ã€è¯—æ­Œè´¨é‡è¯„ä»·/ç»†ç²’åº¦æƒ…æ„Ÿè¯—æ­Œè¯­æ–™åº“ã€å¿«é€Ÿè½¬åŒ–ã€Œä¸­æ–‡æ•°å­—ã€å’Œã€Œé˜¿æ‹‰ä¼¯æ•°å­—ã€ã€ç™¾åº¦çŸ¥é“é—®ç­”è¯­æ–™åº“ã€åŸºäºçŸ¥è¯†å›¾è°±çš„é—®ç­”ç³»ç»Ÿã€jieba_fast åŠ é€Ÿç‰ˆçš„jiebaã€æ­£åˆ™è¡¨è¾¾å¼æ•™ç¨‹ã€ä¸­æ–‡é˜…è¯»ç†è§£æ•°æ®é›†ã€åŸºäºBERTç­‰æœ€æ–°è¯­è¨€æ¨¡å‹çš„æŠ½å–å¼æ‘˜è¦æå–ã€Pythonåˆ©ç”¨æ·±åº¦å­¦ä¹ è¿›è¡Œæ–‡æœ¬æ‘˜è¦çš„ç»¼åˆæŒ‡å—ã€çŸ¥è¯†å›¾è°±æ·±åº¦å­¦ä¹ ç›¸å…³èµ„æ–™æ•´ç†ã€ç»´åŸºå¤§è§„æ¨¡å¹³è¡Œæ–‡æœ¬è¯­æ–™ã€StanfordNLP 0.2.0ï¼šçº¯Pythonç‰ˆè‡ªç„¶è¯­è¨€å¤„ç†åŒ…ã€NeuralNLP-NeuralClassifierï¼šè…¾è®¯å¼€æºæ·±åº¦å­¦ä¹ æ–‡æœ¬åˆ†ç±»å·¥å…·ã€ç«¯åˆ°ç«¯çš„å°é—­åŸŸå¯¹è¯ç³»ç»Ÿã€ä¸­æ–‡å‘½åå®ä½“è¯†åˆ«ï¼šNeuroNER vs. BertNERã€æ–°é—»äº‹ä»¶çº¿ç´¢æŠ½å–ã€2019å¹´ç™¾åº¦çš„ä¸‰å…ƒç»„æŠ½å–æ¯”èµ›ï¼šâ€œç§‘å­¦ç©ºé—´é˜Ÿâ€æºç ã€åŸºäºä¾å­˜å¥æ³•çš„å¼€æ”¾åŸŸæ–‡æœ¬çŸ¥è¯†ä¸‰å…ƒç»„æŠ½å–å’ŒçŸ¥è¯†åº“æ„å»ºã€ä¸­æ–‡çš„GPT2è®­ç»ƒä»£ç ã€ML-NLP - æœºå™¨å­¦ä¹ (Machine Learning)NLPé¢è¯•ä¸­å¸¸è€ƒåˆ°çš„çŸ¥è¯†ç‚¹å’Œä»£ç å®ç°ã€nlp4han:ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·é›†(æ–­å¥/åˆ†è¯/è¯æ€§æ ‡æ³¨/ç»„å—/å¥æ³•åˆ†æ/è¯­ä¹‰åˆ†æ/NER/Nå…ƒè¯­æ³•/HMM/ä»£è¯æ¶ˆè§£/æƒ…æ„Ÿåˆ†æ/æ‹¼å†™æ£€æŸ¥ã€XLMï¼šFacebookçš„è·¨è¯­è¨€é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€ç”¨åŸºäºBERTçš„å¾®è°ƒå’Œç‰¹å¾æå–æ–¹æ³•æ¥è¿›è¡ŒçŸ¥è¯†å›¾è°±ç™¾åº¦ç™¾ç§‘äººç‰©è¯æ¡å±æ€§æŠ½å–ã€ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†ç›¸å…³çš„å¼€æ”¾ä»»åŠ¡-æ•°æ®é›†-å½“å‰æœ€ä½³ç»“æœã€CoupletAI - åŸºäºCNN+Bi-LSTM+Attention çš„è‡ªåŠ¨å¯¹å¯¹è”ç³»ç»Ÿã€æŠ½è±¡çŸ¥è¯†å›¾è°±ã€MiningZhiDaoQACorpus - 580ä¸‡ç™¾åº¦çŸ¥é“é—®ç­”æ•°æ®æŒ–æ˜é¡¹ç›®ã€brat rapid annotation tool: åºåˆ—æ ‡æ³¨å·¥å…·ã€å¤§è§„æ¨¡ä¸­æ–‡çŸ¥è¯†å›¾è°±æ•°æ®ï¼š1.4äº¿å®ä½“ã€æ•°æ®å¢å¼ºåœ¨æœºå™¨ç¿»è¯‘åŠå…¶ä»–nlpä»»åŠ¡ä¸­çš„åº”ç”¨åŠæ•ˆæœã€allennlpé˜…è¯»ç†è§£:æ”¯æŒå¤šç§æ•°æ®å’Œæ¨¡å‹ã€PDFè¡¨æ ¼æ•°æ®æå–å·¥å…· ã€ Graphbrainï¼šAIå¼€æºè½¯ä»¶åº“å’Œç§‘ç ”å·¥å…·ï¼Œç›®çš„æ˜¯ä¿ƒè¿›è‡ªåŠ¨æ„ä¹‰æå–å’Œæ–‡æœ¬ç†è§£ä»¥åŠçŸ¥è¯†çš„æ¢ç´¢å’Œæ¨æ–­ã€ç®€å†è‡ªåŠ¨ç­›é€‰ç³»ç»Ÿã€åŸºäºå‘½åå®ä½“è¯†åˆ«çš„ç®€å†è‡ªåŠ¨æ‘˜è¦ã€ä¸­æ–‡è¯­è¨€ç†è§£æµ‹è¯„åŸºå‡†ï¼ŒåŒ…æ‹¬ä»£è¡¨æ€§çš„æ•°æ®é›†&åŸºå‡†æ¨¡å‹&è¯­æ–™åº“&æ’è¡Œæ¦œã€æ ‘æ´ OCR æ–‡å­—è¯†åˆ« ã€ä»åŒ…å«è¡¨æ ¼çš„æ‰«æå›¾ç‰‡ä¸­è¯†åˆ«è¡¨æ ¼å’Œæ–‡å­—ã€è¯­å£°è¿ç§»ã€Pythonå£è¯­è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·é›†(è‹±æ–‡)ã€ similarityï¼šç›¸ä¼¼åº¦è®¡ç®—å·¥å…·åŒ…ï¼Œjavaç¼–å†™ã€æµ·é‡ä¸­æ–‡é¢„è®­ç»ƒALBERTæ¨¡å‹ ã€Transformers 2.0 ã€åŸºäºå¤§è§„æ¨¡éŸ³é¢‘æ•°æ®é›†Audiosetçš„éŸ³é¢‘å¢å¼º ã€Poplarï¼šç½‘é¡µç‰ˆè‡ªç„¶è¯­è¨€æ ‡æ³¨å·¥å…·ã€å›¾ç‰‡æ–‡å­—å»é™¤ï¼Œå¯ç”¨äºæ¼«ç”»ç¿»è¯‘ ã€186ç§è¯­è¨€çš„æ•°å­—å«æ³•åº“ã€Amazonå‘å¸ƒåŸºäºçŸ¥è¯†çš„äºº-äººå¼€æ”¾é¢†åŸŸå¯¹è¯æ•°æ®é›† ã€ä¸­æ–‡æ–‡æœ¬çº é”™æ¨¡å—ä»£ç ã€ç¹ç®€ä½“è½¬æ¢ ã€ Pythonå®ç°çš„å¤šç§æ–‡æœ¬å¯è¯»æ€§è¯„ä»·æŒ‡æ ‡ã€ç±»ä¼¼äºäººå/åœ°å/ç»„ç»‡æœºæ„åçš„å‘½åä½“è¯†åˆ«æ•°æ®é›† ã€ä¸œå—å¤§å­¦ã€ŠçŸ¥è¯†å›¾è°±ã€‹ç ”ç©¶ç”Ÿè¯¾ç¨‹(èµ„æ–™)ã€. è‹±æ–‡æ‹¼å†™æ£€æŸ¥åº“ ã€ wwsearchæ˜¯ä¼ä¸šå¾®ä¿¡åå°è‡ªç ”çš„å…¨æ–‡æ£€ç´¢å¼•æ“ã€CHAMELEONï¼šæ·±åº¦å­¦ä¹ æ–°é—»æ¨èç³»ç»Ÿå…ƒæ¶æ„ ã€ 8ç¯‡è®ºæ–‡æ¢³ç†BERTç›¸å…³æ¨¡å‹è¿›å±•ä¸åæ€ã€DocSearchï¼šå…è´¹æ–‡æ¡£æœç´¢å¼•æ“ã€ LIDAï¼šè½»é‡äº¤äº’å¼å¯¹è¯æ ‡æ³¨å·¥å…· ã€aili - the fastest in-memory index in the East ä¸œåŠçƒæœ€å¿«å¹¶å‘ç´¢å¼• ã€çŸ¥è¯†å›¾è°±è½¦éŸ³å·¥ä½œé¡¹ç›®ã€è‡ªç„¶è¯­è¨€ç”Ÿæˆèµ„æºå¤§å…¨ ã€ä¸­æ—¥éŸ©åˆ†è¯åº“mecabçš„Pythonæ¥å£åº“ã€ä¸­æ–‡æ–‡æœ¬æ‘˜è¦/å…³é”®è¯æå–ã€æ±‰å­—å­—ç¬¦ç‰¹å¾æå–å™¨ (featurizer)ï¼Œæå–æ±‰å­—çš„ç‰¹å¾ï¼ˆå‘éŸ³ç‰¹å¾ã€å­—å½¢ç‰¹å¾ï¼‰ç”¨åšæ·±åº¦å­¦ä¹ çš„ç‰¹å¾ã€ä¸­æ–‡ç”Ÿæˆä»»åŠ¡åŸºå‡†æµ‹è¯„ ã€ä¸­æ–‡ç¼©å†™æ•°æ®é›†ã€ä¸­æ–‡ä»»åŠ¡åŸºå‡†æµ‹è¯„ - ä»£è¡¨æ€§çš„æ•°æ®é›†-åŸºå‡†(é¢„è®­ç»ƒ)æ¨¡å‹-è¯­æ–™åº“-baseline-å·¥å…·åŒ…-æ’è¡Œæ¦œã€PySS3ï¼šé¢å‘å¯è§£é‡ŠAIçš„SS3æ–‡æœ¬åˆ†ç±»å™¨æœºå™¨å¯è§†åŒ–å·¥å…· ã€ä¸­æ–‡NLPæ•°æ®é›†åˆ—è¡¨ã€COPE - æ ¼å¾‹è¯—ç¼–è¾‘ç¨‹åºã€doccanoï¼šåŸºäºç½‘é¡µçš„å¼€æºååŒå¤šè¯­è¨€æ–‡æœ¬æ ‡æ³¨å·¥å…· ã€PreNLPï¼šè‡ªç„¶è¯­è¨€é¢„å¤„ç†åº“ã€ç®€å•çš„ç®€å†è§£æå™¨ï¼Œç”¨æ¥ä»ç®€å†ä¸­æå–å…³é”®ä¿¡æ¯ã€ç”¨äºä¸­æ–‡é—²èŠçš„GPT2æ¨¡å‹ï¼šGPT2-chitchatã€åŸºäºæ£€ç´¢èŠå¤©æœºå™¨äººå¤šè½®å“åº”é€‰æ‹©ç›¸å…³èµ„æºåˆ—è¡¨(Leaderboardsã€Datasetsã€Papers)ã€(Colab)æŠ½è±¡æ–‡æœ¬æ‘˜è¦å®ç°é›†é”¦(æ•™ç¨‹ ã€è¯è¯­æ‹¼éŸ³æ•°æ®ã€é«˜æ•ˆæ¨¡ç³Šæœç´¢å·¥å…·ã€NLPæ•°æ®å¢å¹¿èµ„æºé›†ã€å¾®è½¯å¯¹è¯æœºå™¨äººæ¡†æ¶ ã€ GitHub Typo Corpusï¼šå¤§è§„æ¨¡GitHubå¤šè¯­è¨€æ‹¼å†™é”™è¯¯/è¯­æ³•é”™è¯¯æ•°æ®é›†ã€TextClusterï¼šçŸ­æ–‡æœ¬èšç±»é¢„å¤„ç†æ¨¡å— Short text clusterã€é¢å‘è¯­éŸ³è¯†åˆ«çš„ä¸­æ–‡æ–‡æœ¬è§„èŒƒåŒ–ã€BLINKï¼šæœ€å…ˆè¿›çš„å®ä½“é“¾æ¥åº“ã€BertPuncï¼šåŸºäºBERTçš„æœ€å…ˆè¿›æ ‡ç‚¹ä¿®å¤æ¨¡å‹ã€Tokenizerï¼šå¿«é€Ÿã€å¯å®šåˆ¶çš„æ–‡æœ¬è¯æ¡åŒ–åº“ã€ä¸­æ–‡è¯­è¨€ç†è§£æµ‹è¯„åŸºå‡†ï¼ŒåŒ…æ‹¬ä»£è¡¨æ€§çš„æ•°æ®é›†ã€åŸºå‡†(é¢„è®­ç»ƒ)æ¨¡å‹ã€è¯­æ–™åº“ã€æ’è¡Œæ¦œã€spaCy åŒ»å­¦æ–‡æœ¬æŒ–æ˜ä¸ä¿¡æ¯æå– ã€ NLPä»»åŠ¡ç¤ºä¾‹é¡¹ç›®ä»£ç é›†ã€ pythonæ‹¼å†™æ£€æŸ¥åº“ã€chatbot-list - è¡Œä¸šå†…å…³äºæ™ºèƒ½å®¢æœã€èŠå¤©æœºå™¨äººçš„åº”ç”¨å’Œæ¶æ„ã€ç®—æ³•åˆ†äº«å’Œä»‹ç»ã€è¯­éŸ³è´¨é‡è¯„ä»·æŒ‡æ ‡(MOSNet, BSSEval, STOI, PESQ, SRMR)ã€ ç”¨138GBè¯­æ–™è®­ç»ƒçš„æ³•æ–‡RoBERTaé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ ã€BERT-NER-Pytorchï¼šä¸‰ç§ä¸åŒæ¨¡å¼çš„BERTä¸­æ–‡NERå®éªŒã€æ— é“è¯å…¸ - æœ‰é“è¯å…¸çš„å‘½ä»¤è¡Œç‰ˆæœ¬ï¼Œæ”¯æŒè‹±æ±‰äº’æŸ¥å’Œåœ¨çº¿æŸ¥è¯¢ã€2019å¹´NLPäº®ç‚¹å›é¡¾ã€ Chinese medical dialogue data ä¸­æ–‡åŒ»ç–—å¯¹è¯æ•°æ®é›† ã€æœ€å¥½çš„æ±‰å­—æ•°å­—(ä¸­æ–‡æ•°å­—)-é˜¿æ‹‰ä¼¯æ•°å­—è½¬æ¢å·¥å…·ã€ åŸºäºç™¾ç§‘çŸ¥è¯†åº“çš„ä¸­æ–‡è¯è¯­å¤šè¯ä¹‰/ä¹‰é¡¹è·å–ä¸ç‰¹å®šå¥å­è¯è¯­è¯­ä¹‰æ¶ˆæ­§ã€awesome-nlp-sentiment-analysis - æƒ…æ„Ÿåˆ†æã€æƒ…ç»ªåŸå› è¯†åˆ«ã€è¯„ä»·å¯¹è±¡å’Œè¯„ä»·è¯æŠ½å–ã€LineFlowï¼šé¢å‘æ‰€æœ‰æ·±åº¦å­¦ä¹ æ¡†æ¶çš„NLPæ•°æ®é«˜æ•ˆåŠ è½½å™¨ã€ä¸­æ–‡åŒ»å­¦NLPå…¬å¼€èµ„æºæ•´ç† ã€MedQuADï¼š(è‹±æ–‡)åŒ»å­¦é—®ç­”æ•°æ®é›†ã€å°†è‡ªç„¶è¯­è¨€æ•°å­—ä¸²è§£æè½¬æ¢ä¸ºæ•´æ•°å’Œæµ®ç‚¹æ•°ã€Transfer Learning in Natural Language Processing (NLP) ã€é¢å‘è¯­éŸ³è¯†åˆ«çš„ä¸­æ–‡/è‹±æ–‡å‘éŸ³è¾å…¸ã€Tokenizersï¼šæ³¨é‡æ€§èƒ½ä¸å¤šåŠŸèƒ½æ€§çš„æœ€å…ˆè¿›åˆ†è¯å™¨ã€CLUENER ç»†ç²’åº¦å‘½åå®ä½“è¯†åˆ« Fine Grained Named Entity Recognitionã€ åŸºäºBERTçš„ä¸­æ–‡å‘½åå®ä½“è¯†åˆ«ã€ä¸­æ–‡è°£è¨€æ•°æ®åº“ã€NLPæ•°æ®é›†/åŸºå‡†ä»»åŠ¡å¤§åˆ—è¡¨ã€nlpç›¸å…³çš„ä¸€äº›è®ºæ–‡åŠä»£ç , åŒ…æ‹¬ä¸»é¢˜æ¨¡å‹ã€è¯å‘é‡(Word Embedding)ã€å‘½åå®ä½“è¯†åˆ«(NER)ã€æ–‡æœ¬åˆ†ç±»(Text Classificatin)ã€æ–‡æœ¬ç”Ÿæˆ(Text Generation)ã€æ–‡æœ¬ç›¸ä¼¼æ€§(Text Similarity)è®¡ç®—ç­‰ï¼Œæ¶‰åŠåˆ°å„ç§ä¸nlpç›¸å…³çš„ç®—æ³•ï¼ŒåŸºäºkeraså’Œtensorflow ã€Pythonæ–‡æœ¬æŒ–æ˜/NLPå®æˆ˜ç¤ºä¾‹ã€ Blackstoneï¼šé¢å‘éç»“æ„åŒ–æ³•å¾‹æ–‡æœ¬çš„spaCy pipelineå’ŒNLPæ¨¡å‹é€šè¿‡åŒä¹‰è¯æ›¿æ¢å®ç°æ–‡æœ¬â€œå˜è„¸â€ ã€ä¸­æ–‡ é¢„è®­ç»ƒ ELECTREA æ¨¡å‹: åŸºäºå¯¹æŠ—å­¦ä¹  pretrain Chinese Model ã€albert-chinese-ner - ç”¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ALBERTåšä¸­æ–‡NER ã€åŸºäºGPT2çš„ç‰¹å®šä¸»é¢˜æ–‡æœ¬ç”Ÿæˆ/æ–‡æœ¬å¢å¹¿ã€å¼€æºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åˆé›†ã€å¤šè¯­è¨€å¥å‘é‡åŒ…ã€ç¼–ç ã€æ ‡è®°å’Œå®ç°ï¼šä¸€ç§å¯æ§é«˜æ•ˆçš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³•ã€ è‹±æ–‡è„è¯å¤§åˆ—è¡¨ ã€attnvisï¼šGPT2ã€BERTç­‰transformerè¯­è¨€æ¨¡å‹æ³¨æ„åŠ›äº¤äº’å¯è§†åŒ–ã€CoVoSTï¼šFacebookå‘å¸ƒçš„å¤šè¯­ç§è¯­éŸ³-æ–‡æœ¬ç¿»è¯‘è¯­æ–™åº“ï¼ŒåŒ…æ‹¬11ç§è¯­è¨€(æ³•è¯­ã€å¾·è¯­ã€è·å…°è¯­ã€ä¿„è¯­ã€è¥¿ç­ç‰™è¯­ã€æ„å¤§åˆ©è¯­ã€åœŸè€³å…¶è¯­ã€æ³¢æ–¯è¯­ã€ç‘å…¸è¯­ã€è’™å¤è¯­å’Œä¸­æ–‡)çš„è¯­éŸ³ã€æ–‡å­—è½¬å½•åŠè‹±æ–‡è¯‘æ–‡ã€Jiaguè‡ªç„¶è¯­è¨€å¤„ç†å·¥å…· - ä»¥BiLSTMç­‰æ¨¡å‹ä¸ºåŸºç¡€ï¼Œæä¾›çŸ¥è¯†å›¾è°±å…³ç³»æŠ½å– ä¸­æ–‡åˆ†è¯ è¯æ€§æ ‡æ³¨ å‘½åå®ä½“è¯†åˆ« æƒ…æ„Ÿåˆ†æ æ–°è¯å‘ç° å…³é”®è¯ æ–‡æœ¬æ‘˜è¦ æ–‡æœ¬èšç±»ç­‰åŠŸèƒ½ã€ç”¨unetå®ç°å¯¹æ–‡æ¡£è¡¨æ ¼çš„è‡ªåŠ¨æ£€æµ‹ï¼Œè¡¨æ ¼é‡å»ºã€NLPäº‹ä»¶æå–æ–‡çŒ®èµ„æºåˆ—è¡¨ ã€ é‡‘èé¢†åŸŸè‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶èµ„æºå¤§åˆ—è¡¨ã€CLUEDatasetSearch - ä¸­è‹±æ–‡NLPæ•°æ®é›†ï¼šæœç´¢æ‰€æœ‰ä¸­æ–‡NLPæ•°æ®é›†ï¼Œé™„å¸¸ç”¨è‹±æ–‡NLPæ•°æ®é›† ã€medical_NER - ä¸­æ–‡åŒ»å­¦çŸ¥è¯†å›¾è°±å‘½åå®ä½“è¯†åˆ« ã€(å“ˆä½›)è®²å› æœæ¨ç†çš„å…è´¹ä¹¦ã€çŸ¥è¯†å›¾è°±ç›¸å…³å­¦ä¹ èµ„æ–™/æ•°æ®é›†/å·¥å…·èµ„æºå¤§åˆ—è¡¨ã€Forteï¼šçµæ´»å¼ºå¤§çš„è‡ªç„¶è¯­è¨€å¤„ç†pipelineå·¥å…·é›† ã€Pythonå­—ç¬¦ä¸²ç›¸ä¼¼æ€§ç®—æ³•åº“ã€PyLaiaï¼šé¢å‘æ‰‹å†™æ–‡æ¡£åˆ†æçš„æ·±åº¦å­¦ä¹ å·¥å…·åŒ…ã€TextFoolerï¼šé’ˆå¯¹æ–‡æœ¬åˆ†ç±»/æ¨ç†çš„å¯¹æŠ—æ–‡æœ¬ç”Ÿæˆæ¨¡å—ã€Haystackï¼šçµæ´»ã€å¼ºå¤§çš„å¯æ‰©å±•é—®ç­”(QA)æ¡†æ¶ã€ä¸­æ–‡å…³é”®çŸ­è¯­æŠ½å–å·¥å…· | 2024-05-10T07:38:24Z |
| 3 | [annotated_deep_learning_paper_implementations](https://github.com/labmlai/annotated_deep_learning_paper_implementations) | 61521 | 6220 | Python | 31 | ğŸ§‘â€ğŸ« 60+ Implementations/tutorials of deep learning papers with side-by-side notes ğŸ“; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, sophia, ...), gans(cyclegan, stylegan2, ...), ğŸ® reinforcement learning (ppo, dqn), capsnet, distillation, ... ğŸ§  | 2024-08-24T09:18:59Z |
| 4 | [LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch) | 57658 | 8014 | Jupyter Notebook | 2 | Implement a ChatGPT-like LLM in PyTorch from scratch, step by step | 2025-06-30T22:49:53Z |
| 5 | [vllm](https://github.com/vllm-project/vllm) | 51096 | 8425 | Python | 1876 | A high-throughput and memory-efficient inference and serving engine for LLMs | 2025-07-01T04:03:55Z |
| 6 | [whisper.cpp](https://github.com/ggml-org/whisper.cpp) | 41179 | 4408 | C++ | 900 | Port of OpenAI's Whisper model in C/C++ | 2025-06-27T13:43:56Z |
| 7 | [pytorch-image-models](https://github.com/huggingface/pytorch-image-models) | 34605 | 4937 | Python | 51 | The largest collection of PyTorch image encoders / backbones. Including train, eval, inference, export scripts, and pretrained weights -- ResNet, ResNeXT, EfficientNet, NFNet, Vision Transformer (ViT), MobileNetV4, MobileNet-V3 & V2, RegNet, DPN, CSPNet, Swin Transformer, MaxViT, CoAtNet, ConvNeXt, and more | 2025-06-30T20:54:48Z |
| 8 | [LocalAI](https://github.com/mudler/LocalAI) | 33587 | 2595 | Go | 444 | :robot: The free, Open Source alternative to OpenAI, Claude and others. Self-hosted and local-first. Drop-in replacement for OpenAI,  running on consumer-grade hardware. No GPU required. Runs gguf, transformers, diffusers and many more models architectures. Features: Generate Text, Audio, Video, Images, Voice Cloning, Distributed, P2P inference | 2025-06-30T23:00:40Z |
| 9 | [mmdetection](https://github.com/open-mmlab/mmdetection) | 31252 | 9688 | Python | 1740 | OpenMMLab Detection Toolbox and Benchmark | 2024-08-21T02:01:07Z |
| 10 | [vit-pytorch](https://github.com/lucidrains/vit-pytorch) | 23254 | 3325 | Python | 128 | Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch | 2025-03-05T18:50:39Z |
| 11 | [minGPT](https://github.com/karpathy/minGPT) | 22172 | 2875 | Python | 50 | A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training | 2024-08-15T04:09:40Z |
| 12 | [fish-speech](https://github.com/fishaudio/fish-speech) | 22132 | 1817 | Python | 38 | SOTA Open Source TTS | 2025-06-12T09:48:29Z |
| 13 | [best-of-ml-python](https://github.com/ml-tooling/best-of-ml-python) | 21384 | 2877 | None | 24 | ğŸ† A ranked list of awesome machine learning Python libraries. Updated weekly. | 2025-06-26T15:25:35Z |
| 14 | [CVPR2025-Papers-with-Code](https://github.com/amusi/CVPR2025-Papers-with-Code) | 20346 | 2696 | None | 1 | CVPR 2025 è®ºæ–‡å’Œå¼€æºé¡¹ç›®åˆé›† | 2025-06-05T05:45:18Z |
| 15 | [CodeFormer](https://github.com/sczhou/CodeFormer) | 17248 | 3575 | Python | 256 | [NeurIPS 2022] Towards Robust Blind Face Restoration with Codebook Lookup Transformer | 2024-10-09T20:31:41Z |
| 16 | [sentence-transformers](https://github.com/UKPLab/sentence-transformers) | 17030 | 2624 | Python | 1238 | State-of-the-Art Text Embeddings | 2025-06-30T16:54:40Z |
| 17 | [faster-whisper](https://github.com/SYSTRAN/faster-whisper) | 16822 | 1386 | Python | 249 | Faster Whisper transcription with CTranslate2 | 2025-06-02T11:12:34Z |
| 18 | [sglang](https://github.com/sgl-project/sglang) | 15588 | 2223 | Python | 490 | SGLang is a fast serving framework for large language models and vision language models. | 2025-07-01T03:30:34Z |
| 19 | [leedl-tutorial](https://github.com/datawhalechina/leedl-tutorial) | 15349 | 3043 | Jupyter Notebook | 2 | ã€Šæå®æ¯…æ·±åº¦å­¦ä¹ æ•™ç¨‹ã€‹ï¼ˆæå®æ¯…è€å¸ˆæ¨èğŸ‘ï¼Œè‹¹æœä¹¦ğŸï¼‰ï¼ŒPDFä¸‹è½½åœ°å€ï¼šhttps://github.com/datawhalechina/leedl-tutorial/releases | 2025-06-13T15:25:49Z |
| 20 | [Swin-Transformer](https://github.com/microsoft/Swin-Transformer) | 14952 | 2151 | Python | 185 | This is an official implementation for "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows". | 2024-07-24T17:09:57Z |
| 21 | [nlp-tutorial](https://github.com/graykode/nlp-tutorial) | 14655 | 3966 | Jupyter Notebook | 32 | Natural Language Processing Tutorial for Deep Learning Researchers | 2024-02-21T13:49:10Z |
| 22 | [LaTeX-OCR](https://github.com/lukas-blecher/LaTeX-OCR) | 14622 | 1169 | Python | 134 | pix2tex: Using a ViT to convert images of equations into LaTeX code. | 2025-01-18T15:23:58Z |
| 23 | [detr](https://github.com/facebookresearch/detr) | 14480 | 2571 | Python | 240 | End-to-End Object Detection with Transformers | 2024-03-12T15:58:25Z |
| 24 | [trl](https://github.com/huggingface/trl) | 14393 | 1997 | Python | 417 | Train transformer language models with reinforcement learning. | 2025-06-28T05:19:01Z |
| 25 | [transformers.js](https://github.com/huggingface/transformers.js) | 13936 | 944 | JavaScript | 339 | State-of-the-art Machine Learning for the web. Run ğŸ¤— Transformers directly in your browser, with no need for a server! | 2025-07-01T03:36:17Z |
| 26 | [RWKV-LM](https://github.com/BlinkDL/RWKV-LM) | 13751 | 920 | Python | 109 | RWKV (pronounced RwaKuv) is an RNN with great LLM performance, which can also be directly trained like a GPT transformer (parallelizable). We are at RWKV-7 "Goose". So it's combining the best of RNN and transformer - great performance, linear time, constant space (no kv-cache), fast training, infinite ctx_len, and free sentence embedding. | 2025-06-30T12:14:43Z |
| 27 | [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) | 12709 | 2888 | Python | 321 | Ongoing research training transformer models at scale | 2025-07-01T02:03:20Z |
| 28 | [dio](https://github.com/cfug/dio) | 12663 | 1535 | Dart | 23 | A powerful HTTP client for Dart and Flutter, which supports global settings, Interceptors, FormData, aborting and canceling a request, files uploading and downloading, requests timeout, custom adapters, etc. | 2025-06-30T17:17:42Z |
| 29 | [MNN](https://github.com/alibaba/MNN) | 12141 | 1966 | C++ | 102 | MNN is a blazing fast, lightweight deep learning framework, battle-tested by business-critical use cases in Alibaba. Full multimodal LLM Android App:[MNN-LLM-Android](./apps/Android/MnnLlmChat/README.md). MNN TaoAvatar Android - Local 3D Avatar Intelligence: apps/Android/Mnn3dAvatar/README.md | 2025-06-30T03:33:57Z |
| 30 | [PaddleSpeech](https://github.com/PaddlePaddle/PaddleSpeech) | 12036 | 1924 | Python | 329 | Easy-to-use Speech Toolkit including Self-Supervised Learning model, SOTA/Streaming ASR with punctuation, Streaming TTS with text frontend, Speaker Verification System, End-to-End Speech Translation and Keyword Spotting. Won NAACL2022 Best Demo Award. | 2025-06-26T04:52:42Z |
| 31 | [pytorch-grad-cam](https://github.com/jacobgil/pytorch-grad-cam) | 11831 | 1645 | Python | 149 | Advanced AI Explainability for computer vision.  Support for CNNs, Vision Transformers, Classification, Object detection, Segmentation, Image similarity and more. | 2025-04-07T05:12:45Z |
| 32 | [vision_transformer](https://github.com/google-research/vision_transformer) | 11510 | 1392 | Jupyter Notebook | 125 | None | 2025-03-06T03:14:39Z |
| 33 | [Transformers-Tutorials](https://github.com/NielsRogge/Transformers-Tutorials) | 11018 | 1640 | Jupyter Notebook | 311 | This repository contains demos I made with the Transformers library by HuggingFace. | 2025-05-26T12:52:16Z |
| 34 | [segmentation_models.pytorch](https://github.com/qubvel-org/segmentation_models.pytorch) | 10618 | 1758 | Python | 59 | Semantic segmentation models with 500+ pretrained convolutional and transformer-based backbones. | 2025-06-27T10:25:05Z |
| 35 | [text-generation-inference](https://github.com/huggingface/text-generation-inference) | 10269 | 1205 | Python | 258 | Large Language Model Text Generation Inference | 2025-06-27T13:14:26Z |
| 36 | [petals](https://github.com/bigscience-workshop/petals) | 9696 | 561 | Python | 92 | ğŸŒ¸ Run LLMs at home, BitTorrent-style. Fine-tuning and inference up to 10x faster than offloading | 2024-09-07T11:54:28Z |
| 37 | [xformers](https://github.com/facebookresearch/xformers) | 9656 | 687 | Python | 319 | Hackable and optimized Transformers building blocks, supporting a composable construction. | 2025-06-25T12:16:59Z |
| 38 | [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) | 9416 | 2494 | Python | 422 | A framework for few-shot evaluation of language models. | 2025-06-30T16:25:37Z |
| 39 | [attention-is-all-you-need-pytorch](https://github.com/jadore801120/attention-is-all-you-need-pytorch) | 9257 | 2028 | Python | 66 | A PyTorch implementation of the Transformer model in "Attention is All You Need". | 2024-04-16T07:27:13Z |
| 40 | [vggt](https://github.com/facebookresearch/vggt) | 9236 | 887 | Python | 100 | [CVPR 2025 Best Paper Award] VGGT: Visual Geometry Grounded Transformer | 2025-06-30T15:33:24Z |
| 41 | [PaddleSeg](https://github.com/PaddlePaddle/PaddleSeg) | 9074 | 1699 | Python | 16 | Easy-to-use image segmentation library with awesome pre-trained model zoo, supporting wide-range of practical tasks in Semantic Segmentation, Interactive Segmentation, Panoptic Segmentation, Image Matting, 3D Segmentation, etc. | 2025-06-10T02:26:07Z |
| 42 | [mmsegmentation](https://github.com/open-mmlab/mmsegmentation) | 9003 | 2734 | Python | 764 | OpenMMLab Semantic Segmentation Toolbox and Benchmark. | 2024-08-13T08:53:34Z |
| 43 | [LMFlow](https://github.com/OptimalScale/LMFlow) | 8439 | 835 | Python | 73 | An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. Large Models for All. | 2025-05-15T09:24:46Z |
| 44 | [trax](https://github.com/google/trax) | 8225 | 828 | Python | 107 | Trax â€” Deep Learning with Clear Code and Speed | 2025-04-10T21:24:04Z |
| 45 | [manga-image-translator](https://github.com/zyddnys/manga-image-translator) | 8016 | 773 | Python | 249 | Translate manga/image ä¸€é”®ç¿»è¯‘å„ç±»å›¾ç‰‡å†…æ–‡å­— https://cotrans.touhou.ai/ | 2025-06-29T15:08:25Z |
| 46 | [jukebox](https://github.com/openai/jukebox) | 8002 | 1448 | Python | 193 | Code for the paper "Jukebox: A Generative Model for Music" | 2024-06-19T05:14:24Z |
| 47 | [GPT2-Chinese](https://github.com/Morizeyao/GPT2-Chinese) | 7579 | 1707 | Python | 100 | Chinese version of GPT2 training code, using BERT tokenizer. | 2024-04-25T09:14:25Z |
| 48 | [bertviz](https://github.com/jessevig/bertviz) | 7502 | 830 | Python | 20 | BertViz: Visualize Attention in NLP Models (BERT, GPT2, BART, etc.)  | 2025-06-01T14:38:39Z |
| 49 | [DiT](https://github.com/facebookresearch/DiT) | 7488 | 662 | Python | 67 | Official PyTorch Implementation of "Scalable Diffusion Models with Transformers" | 2024-05-31T13:04:15Z |
| 50 | [gpt-neox](https://github.com/EleutherAI/gpt-neox) | 7245 | 1065 | Python | 61 | An implementation of model parallel autoregressive transformers on GPUs, based on the Megatron and DeepSpeed libraries | 2025-06-21T06:55:38Z |
| 51 | [class-transformer](https://github.com/typestack/class-transformer) | 7161 | 516 | TypeScript | 201 | Decorator-based transformation, serialization, and deserialization between objects and classes.  | 2025-01-17T13:03:28Z |
| 52 | [lightningcss](https://github.com/parcel-bundler/lightningcss) | 7091 | 215 | Rust | 250 | An extremely fast CSS parser, transformer, bundler, and minifier written in Rust. | 2025-06-24T18:19:24Z |
| 53 | [ts-jest](https://github.com/kulshekhar/ts-jest) | 7045 | 462 | TypeScript | 68 | A Jest transformer with source map support that lets you use Jest to test projects written in TypeScript. | 2025-06-30T20:33:37Z |
| 54 | [dino](https://github.com/facebookresearch/dino) | 6938 | 958 | Python | 100 | PyTorch code for Vision Transformers training with the Self-Supervised learning method DINO | 2024-07-03T16:21:59Z |
| 55 | [MindSearch](https://github.com/InternLM/MindSearch) | 6424 | 655 | JavaScript | 41 | ğŸ” An LLM-based Multi-agent Framework of Web Search Engine (like Perplexity.ai Pro and SearchGPT) | 2025-01-08T09:34:38Z |
| 56 | [BERT-pytorch](https://github.com/codertimo/BERT-pytorch) | 6422 | 1322 | Python | 56 | Google AI 2018 BERT pytorch implementation | 2023-09-15T12:57:08Z |
| 57 | [text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer) | 6384 | 776 | Python | 59 | Code for the paper "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" | 2025-04-28T17:33:03Z |
| 58 | [donut](https://github.com/clovaai/donut) | 6370 | 519 | Python | 205 | Official Implementation of OCR-free Document Understanding Transformer (Donut) and Synthetic Document Generator (SynthDoG), ECCV 2022 | 2024-07-11T15:33:26Z |
| 59 | [mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax) | 6342 | 887 | Python | 48 | Model parallel transformers in JAX and Haiku | 2023-01-21T00:09:29Z |
| 60 | [annotated-transformer](https://github.com/harvardnlp/annotated-transformer) | 6315 | 1367 | Jupyter Notebook | 29 | An annotated implementation of the Transformer paper. | 2024-04-07T09:58:46Z |
| 61 | [taming-transformers](https://github.com/CompVis/taming-transformers) | 6231 | 1195 | Jupyter Notebook | 147 | Taming Transformers for High-Resolution Image Synthesis | 2024-07-30T18:27:31Z |
| 62 | [FasterTransformer](https://github.com/NVIDIA/FasterTransformer) | 6222 | 909 | C++ | 249 | Transformer related optimization, including BERT, GPT | 2024-03-27T11:25:30Z |
| 63 | [ProPainter](https://github.com/sczhou/ProPainter) | 6133 | 701 | Python | 67 | [ICCV 2023] ProPainter: Improving Propagation and Transformer for Video Inpainting | 2025-02-19T12:07:56Z |
| 64 | [Informer2020](https://github.com/zhouhaoyi/Informer2020) | 6010 | 1247 | Python | 180 | The GitHub repository for the paper "Informer" accepted by AAAI 2021. | 2025-06-20T06:42:54Z |
| 65 | [gpt-fast](https://github.com/pytorch-labs/gpt-fast) | 6002 | 558 | Python | 74 | Simple and efficient pytorch-native transformer text generation in <1000 LOC of python. | 2025-04-11T19:51:07Z |
| 66 | [gogocode](https://github.com/thx/gogocode) | 5971 | 465 | JavaScript | 87 | GoGoCode is a transformer for JavaScript/Typescript/HTML based on AST but providing a more intuitive API. | 2024-04-11T02:24:33Z |
| 67 | [tsai](https://github.com/timeseriesAI/tsai) | 5705 | 693 | Jupyter Notebook | 116 | Time series Timeseries Deep Learning Machine Learning Python Pytorch  fastai \| State-of-the-art Deep Learning library  for Time Series and Sequences in Pytorch / fastai | 2025-03-02T10:12:00Z |
| 68 | [DALLE-pytorch](https://github.com/lucidrains/DALLE-pytorch) | 5617 | 642 | Python | 121 | Implementation / replication of DALL-E, OpenAI's Text to Image Transformer, in Pytorch | 2024-02-17T21:42:10Z |
| 69 | [Chinese-Text-Classification-Pytorch](https://github.com/649453932/Chinese-Text-Classification-Pytorch) | 5611 | 1257 | Python | 81 | ä¸­æ–‡æ–‡æœ¬åˆ†ç±»ï¼ŒTextCNNï¼ŒTextRNNï¼ŒFastTextï¼ŒTextRCNNï¼ŒBiLSTM_Attentionï¼ŒDPCNNï¼ŒTransformerï¼ŒåŸºäºpytorchï¼Œå¼€ç®±å³ç”¨ã€‚ | 2020-09-23T11:28:21Z |
| 70 | [pytorch-seq2seq](https://github.com/bentrevett/pytorch-seq2seq) | 5572 | 1361 | Jupyter Notebook | 5 | Tutorials on implementing a few sequence-to-sequence (seq2seq) models with PyTorch and TorchText. | 2024-01-20T16:51:04Z |
| 71 | [x-transformers](https://github.com/lucidrains/x-transformers) | 5414 | 468 | Python | 65 | A concise but complete full-attention transformer with a set of promising experimental features from various papers | 2025-06-26T15:15:22Z |
| 72 | [bert4keras](https://github.com/bojone/bert4keras) | 5414 | 927 | Python | 165 | keras implement of transformers for humans | 2024-11-11T15:41:47Z |
| 73 | [recast](https://github.com/benjamn/recast) | 5131 | 352 | TypeScript | 166 | JavaScript syntax tree transformer, nondestructive pretty-printer, and automatic source map generator | 2025-03-03T01:52:20Z |
| 74 | [SwinIR](https://github.com/JingyunLiang/SwinIR) | 4948 | 584 | Python | 67 | SwinIR: Image Restoration Using Swin Transformer (official repository) | 2024-05-14T07:05:48Z |
| 75 | [Awesome-Transformer-Attention](https://github.com/cmhungsteve/Awesome-Transformer-Attention) | 4891 | 495 | None | 3 | An ultimately comprehensive paper list of Vision Transformer/Attention, including papers, codes, and related websites | 2024-07-30T06:57:18Z |
| 76 | [AutoGPTQ](https://github.com/AutoGPTQ/AutoGPTQ) | 4878 | 522 | Python | 243 | An easy-to-use LLMs quantization package with user-friendly apis, based on GPTQ algorithm. | 2025-04-11T13:27:20Z |
| 77 | [understand-prompt](https://github.com/phodal/understand-prompt) | 4786 | 398 | Jupyter Notebook | 0 | ã€ğŸ”ğŸ”ğŸ” å†…å«ä¸é€‚åˆæœªæˆå¹´äººé˜…è¯»çš„å›¾ç‰‡ã€‘åŸºäºæˆ‘æ“…é•¿çš„ç¼–ç¨‹ã€ç»˜ç”»ã€å†™ä½œå±•å¼€çš„ AI æ¢ç´¢å’Œæ€»ç»“ï¼šStableDiffusion æ˜¯ä¸€ç§å¼ºå¤§çš„å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œèƒ½å¤Ÿé€šè¿‡å¯¹ä¸€å¼ å›¾ç‰‡è¿›è¡Œæ¼”åŒ–æ¥ç”Ÿæˆæ–°çš„å›¾ç‰‡ã€‚ChatGPT æ˜¯ä¸€ä¸ªåŸºäº Transformer çš„è¯­è¨€ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒèƒ½å¤Ÿè‡ªåŠ¨ä¸ºè¾“å…¥çš„ä¸»é¢˜ç”Ÿæˆåˆé€‚çš„æ–‡ç« ã€‚è€Œ Github Copilot æ˜¯ä¸€ä¸ªæ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹ï¼Œèƒ½å¤ŸåŠ é€Ÿæ—¥å¸¸ç¼–ç¨‹æ´»åŠ¨ã€‚ | 2023-03-11T13:25:16Z |
| 78 | [transformer-explainer](https://github.com/poloclub/transformer-explainer) | 4741 | 477 | JavaScript | 5 | Transformer Explained Visually: Learn How LLM Transformer Models Work with Interactive Visualization | 2025-06-06T17:49:00Z |
| 79 | [OpenPrompt](https://github.com/thunlp/OpenPrompt) | 4655 | 475 | Python | 86 | An Open-Source Framework for Prompt-Learning. | 2024-07-16T03:48:08Z |
| 80 | [Awesome-Prompt-Engineering](https://github.com/promptslab/Awesome-Prompt-Engineering) | 4649 | 439 | Python | 1 | This repository contains a hand-curated resources for Prompt Engineering with a focus on Generative Pre-trained Transformer (GPT), ChatGPT, PaLM etc  | 2024-07-05T17:19:07Z |
| 81 | [wenet](https://github.com/wenet-e2e/wenet) | 4646 | 1142 | Python | 18 | Production First and Production Ready End-to-End Speech Recognition Toolkit | 2025-06-11T07:02:37Z |
| 82 | [primus](https://github.com/primus/primus) | 4477 | 271 | JavaScript | 50 | :zap: Primus, the creator god of the transformers & an abstraction layer for real-time to prevent module lock-in. | 2023-11-06T18:13:11Z |
| 83 | [notebooks](https://github.com/nlp-with-transformers/notebooks) | 4398 | 1365 | Jupyter Notebook | 83 | Jupyter notebooks for the Natural Language Processing with Transformers book | 2024-08-21T08:45:31Z |
| 84 | [transformer](https://github.com/Kyubyong/transformer) | 4382 | 1308 | Python | 126 | A TensorFlow Implementation of the Transformer: Attention Is All You Need | 2023-05-21T17:39:56Z |
| 85 | [Sana](https://github.com/NVlabs/Sana) | 4316 | 277 | Python | 61 | SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformer | 2025-06-28T08:57:57Z |
| 86 | [Efficient-AI-Backbones](https://github.com/huawei-noah/Efficient-AI-Backbones) | 4250 | 723 | Python | 91 | Efficient AI Backbones including GhostNet, TNT and MLP, developed by Huawei Noah's Ark Lab. | 2025-03-15T12:48:07Z |
| 87 | [simpletransformers](https://github.com/ThilinaRajapakse/simpletransformers) | 4185 | 727 | Python | 75 | Transformers for Information Retrieval, Text Classification, NER, QA, Language Modelling, Language Generation, T5, Multi-Modal, and Conversational AI | 2025-04-26T14:04:56Z |
| 88 | [HunyuanDiT](https://github.com/Tencent-Hunyuan/HunyuanDiT) | 4180 | 349 | Jupyter Notebook | 100 | Hunyuan-DiT : A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding | 2025-01-13T03:22:41Z |
| 89 | [qpdf](https://github.com/qpdf/qpdf) | 4092 | 314 | C++ | 133 | qpdf: A content-preserving PDF document transformer | 2025-06-30T17:59:05Z |
| 90 | [transformer-debugger](https://github.com/openai/transformer-debugger) | 4085 | 243 | Python | 9 | None | 2024-06-04T00:21:06Z |
| 91 | [beat-ai](https://github.com/sunface/beat-ai) | 3994 | 220 | Handlebars | 1 | ğŸš€ Beat AI å­¦ä¹ ç¤¾åŒº: æŒç»­åˆ†äº« AI é¢†åŸŸçš„é«˜è´¨é‡è¿›é˜¶çŸ¥è¯†ï¼Œå¸®ä½ å¾æœ AIï¼ŒJust beat it!  æ¬¢è¿ star è®¢é˜…. | 2025-06-19T03:42:02Z |
| 92 | [CTranslate2](https://github.com/OpenNMT/CTranslate2) | 3884 | 368 | C++ | 202 | Fast inference engine for Transformer models | 2025-04-08T19:43:21Z |
| 93 | [RT-DETR](https://github.com/lyuwenyu/RT-DETR) | 3848 | 450 | Python | 382 | [CVPR 2024] Official RT-DETR (RTDETR paddle pytorch), Real-Time DEtection TRansformer, DETRs Beat YOLOs on Real-time Object Detection. ğŸ”¥ ğŸ”¥ ğŸ”¥  | 2025-04-28T02:50:19Z |
| 94 | [regenerator](https://github.com/facebook/regenerator) | 3839 | 1150 | JavaScript | 63 | Source transformer enabling ECMAScript 6 generator functions in JavaScript-of-today. | 2024-02-29T11:04:34Z |
| 95 | [transformer](https://github.com/hyunwoongko/transformer) | 3831 | 546 | Python | 13 | Transformer: PyTorch Implementation of "Attention Is All You Need" | 2024-08-06T14:40:08Z |
| 96 | [transformer-xl](https://github.com/kimiyoung/transformer-xl) | 3660 | 761 | Python | 92 | None | 2022-09-21T06:22:01Z |
| 97 | [neuralforecast](https://github.com/Nixtla/neuralforecast) | 3595 | 429 | Python | 105 | Scalable and user friendly neural :brain: forecasting algorithms. | 2025-06-28T14:37:33Z |
| 98 | [Deformable-DETR](https://github.com/fundamentalvision/Deformable-DETR) | 3593 | 574 | Python | 169 | Deformable DETR: Deformable Transformers for End-to-End Object Detection. | 2024-05-16T03:54:39Z |
| 99 | [Awesome-Visual-Transformer](https://github.com/dk-liang/Awesome-Visual-Transformer) | 3498 | 402 | None | 2 | Collect some papers about transformer with vision. Awesome Transformer with Computer Vision (CV) | 2025-01-07T01:59:49Z |
| 100 | [transformerlab-app](https://github.com/transformerlab/transformerlab-app) | 3459 | 294 | TypeScript | 46 | Open Source Application for Advanced LLM + Diffusion Engineering: interact, train, fine-tune, and evaluate large language models on your own computer. | 2025-06-30T21:37:35Z |

